{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuKObBsQaxdR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import json \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change \"who_is_running_this_code\" before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcOEfWXb_jpY"
   },
   "outputs": [],
   "source": [
    "# Variables.\n",
    "# ------------------------------------------\n",
    "\n",
    "who_is_running_this_code = \"hogni\"\n",
    "library = \"Library\"\n",
    "video_folder = \"16032021\"\n",
    "\n",
    "file_name = \"hogni_30fps_highup_16032010\"\n",
    "parent_parth = f\"/Users/{who_is_running_this_code}/{library}/Mobile Documents/com~apple~CloudDocs/Bachelor Project/Videos/{video_folder}/\"\n",
    "\n",
    "counter_path = f\"{parent_parth}Data/{file_name}/counterData_{file_name}.csv\"\n",
    "tracker_path = f\"{parent_parth}Data/{file_name}/tracker_{file_name}.json\"\n",
    "video_path = f\"{parent_parth}Processed/{file_name}.mp4\"\n",
    "photo_path = f\"{parent_parth}Photos/{file_name}\"\n",
    "save_labels_path = f\"{parent_parth}Data/{file_name}/\"\n",
    "db_path = f\"{parent_parth}Data/{file_name}/db_{file_name}.csv\"\n",
    "distance = 1\n",
    "orientation = 180\n",
    "notherly_correction_factor = 180\n",
    "classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykoo9V71axru"
   },
   "outputs": [],
   "source": [
    "# Velocity Calculations\n",
    "# ------------------------------------------\n",
    "\n",
    "def calculate_velocity(distance, df):\n",
    "    \"\"\"Velocity Calculations: v = S/t \"\"\"\n",
    "    velocit_dictionary = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if row[4] in velocit_dictionary.keys():\n",
    "            velocit_dictionary[row[4]].append(row[1])\n",
    "        else:\n",
    "            velocit_dictionary[row[4]] = [row[1]]\n",
    "\n",
    "    for key, value in velocit_dictionary.copy().items():\n",
    "        if value[0] > value[1]:\n",
    "            difference = (value[0]-value[1])\n",
    "        else:\n",
    "            difference = (value[1]-value[0])\n",
    "        velocit_dictionary[key] = difference\n",
    "\n",
    "    for key, value in velocit_dictionary.copy().items():\n",
    "        t = velocit_dictionary[key].total_seconds()\n",
    "        if t == 0:\n",
    "            t = 1\n",
    "            velocit_dictionary[key] = distance/t\n",
    "        else:\n",
    "            velocit_dictionary[key] = distance/t\n",
    "    return velocit_dictionary\n",
    "\n",
    "# Absolute Heading \n",
    "# ------------------------------------------\n",
    "\n",
    "def calculate_true_heading(frame_orientation, df):\n",
    "    true_heading_dictionary = {}\n",
    "    for index, row in df.iterrows():\n",
    "        true_heading = row[\"bearing_og\"] + frame_orientation\n",
    "        if true_heading > 360:\n",
    "            true_heading = true_heading - 360\n",
    "        if true_heading < 0:\n",
    "            true_heading = true_heading + 360\n",
    "        true_heading_dictionary[row[0]] = true_heading\n",
    "    return true_heading_dictionary\n",
    "\n",
    "# Init DataFrames\n",
    "# ------------------------------------------\n",
    "\n",
    "# Counter\n",
    "def create_counter_df(path, distance, orientation, notherly_correction_factor):\n",
    "    \"\"\"Read and format counter df\"\"\"\n",
    "    columns = [\"frameId\", \"timestamp\", \"counter_area\", \"ObjectClass\", \n",
    "            \"UniqueID\", \"bearing_og\", \"countingDirection\", \"angle\"]\n",
    "\n",
    "    counter = pd.read_csv(path, names=columns)\n",
    "    counter = counter[counter.duplicated(subset=[\"UniqueID\"], keep=False)].sort_values(\"UniqueID\")\n",
    "    counter['timestamp'] = pd.to_datetime(counter['timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    counter = fix_df(counter)\n",
    "    counter = add_velocity_to_df(distance, counter)\n",
    "    counter = add_true_heading_to_df(orientation, counter, notherly_correction_factor)\n",
    "    return counter\n",
    "\n",
    "def add_velocity_to_df(distance, counter_df):\n",
    "    \"\"\" Calculates speeds (and append to df) speeds: {UniqueID: velocity} \"\"\"\n",
    "    speeds = calculate_velocity(distance, counter_df)\n",
    "    for key, value in speeds.items():\n",
    "        counter_df.loc[counter_df[\"UniqueID\"]==key, \"velocity\"]=value\n",
    "    return counter_df\n",
    "\n",
    "def add_true_heading_to_df(orientation, counter_df, notherly_correction_factor):\n",
    "    \"\"\" Calculates true heading (and append to df): {frameId: true_heading} \"\"\"\n",
    "    true_heading = calculate_true_heading(orientation, counter_df)\n",
    "    for key, value in true_heading.items():\n",
    "        counter_df.loc[counter_df[\"frameId\"]==key, \"true_heading\"]=value\n",
    "        corrected_heading = value + notherly_correction_factor\n",
    "        if corrected_heading > 360:\n",
    "            counter_df.loc[counter_df[\"frameId\"]==key, \"adjusted_true_heading\"]= corrected_heading - 360\n",
    "        else:\n",
    "            counter_df.loc[counter_df[\"frameId\"]==key, \"adjusted_true_heading\"]= corrected_heading\n",
    "    return counter_df\n",
    "\n",
    "# Tracker\n",
    "def create_tracker_df(tracker_path):\n",
    "    with open(tracker_path) as f:\n",
    "        tracker = json.load(f)\n",
    "    tracker_flattend = pd.json_normalize(tracker, record_path='objects', meta=['frameId']).rename(columns={'id': 'UniqueID'})\n",
    "    tracker_flattend = tracker_flattend.drop_duplicates(subset=['UniqueID', 'frameId'], keep='first')\n",
    "    return tracker_flattend\n",
    "\n",
    "# Fix inaccurate data\n",
    "# ------------------------------------------\n",
    "\n",
    "def fix_df(counter_df):\n",
    "    error_dict = {}\n",
    "    for index, row in counter_df.iterrows():\n",
    "        if row[\"UniqueID\"] in error_dict:\n",
    "            error_dict[row[\"UniqueID\"]].append([row[\"frameId\"], row[\"bearing_og\"]])\n",
    "        else:\n",
    "            error_dict[row[\"UniqueID\"]] = [[row[\"frameId\"], row[\"bearing_og\"]]]\n",
    "\n",
    "    for key, value in error_dict.items():\n",
    "        if value[0][1] == 270 or value[1][1] == 270:\n",
    "            if value[0][1] == 270:\n",
    "                if value[1][1] < 150 and value[1][1] > 50:\n",
    "                   counter_df.loc[counter_df[\"frameId\"]==error_dict[key][0][0], \"bearing_og\"]=90\n",
    "            else:\n",
    "                if value[0][1] < 150 and value[0][1] > 50:\n",
    "                   counter_df.loc[counter_df[\"frameId\"]==error_dict[key][1][0], \"bearing_og\"]=90\n",
    "    return counter_df\n",
    "\n",
    "# Merge counter and tracker\n",
    "# ------------------------------------------\n",
    "\n",
    "def merge_counter_and_tracker_df(counter_path, tracker_path, distance, orientation, notherly_correction_factor):\n",
    "    counter_df = create_counter_df(counter_path, distance, orientation, notherly_correction_factor)\n",
    "    tracker_df = create_tracker_df(tracker_path)\n",
    "    return pd.merge(counter_df, tracker_df, on=[\"UniqueID\", \"frameId\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2pJ65_2a8aF"
   },
   "outputs": [],
   "source": [
    "# Image Generation\n",
    "# ------------------------------------------\n",
    "\n",
    "def save_frame(frame_number, source, arrows=None):\n",
    "    vc = cv2.VideoCapture(source)\n",
    "    vc.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    rval, frame = vc.read()\n",
    "    if arrows != None:\n",
    "        for a in arrows:\n",
    "            frame = cv2.arrowedLine(frame, a['start'], a['end'], (0,0,255), thickness=8, tipLength=0.6)\n",
    "    cv2.imwrite(f\"{photo_path}/{str(frame_number)}.jpg\", frame)\n",
    "\n",
    "def get_arrow(obj):\n",
    "    x, y = obj['x'], obj['y']\n",
    "    angle = obj['bearing_og']\n",
    "    speed = obj['velocity']\n",
    "    a = 30 * speed * math.sin(math.radians(angle))\n",
    "    b = 30 * speed * math.cos(math.radians(angle))\n",
    "    return {'start': (int(x), int(y)), 'end': (int(x+a), int(y+b))}\n",
    "\n",
    "# Stich images together.\n",
    "# ------------------------------------------\n",
    "\n",
    "def get_image_pairs(merged_df):\n",
    "    image_pairs_dict = {}\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if row[4] in image_pairs_dict:\n",
    "            image_pairs_dict[row[\"UniqueID\"]].append(row[\"frameId\"])\n",
    "        else:\n",
    "            image_pairs_dict[row[\"UniqueID\"]] = [row[\"frameId\"]]\n",
    "    return image_pairs_dict\n",
    "\n",
    "def merge_images(photo_path):\n",
    "    image_pairs_dict = get_image_pairs(merged_df)\n",
    "    for key, value in image_pairs_dict.items():\n",
    "        image1 = Image.open(f\"{photo_path}/{value[0]}.jpg\")\n",
    "        image2 = Image.open(f\"{photo_path}/{value[1]}.jpg\")\n",
    "\n",
    "        (width1, height1) = image1.size\n",
    "\n",
    "        result_width = width1 * 2\n",
    "        result_height = height1\n",
    "\n",
    "        result = Image.new('RGB', (result_width, result_height))\n",
    "        result.paste(im=image1, box=(0, 0))\n",
    "        result.paste(im=image2, box=(width1, 0))\n",
    "        result.save(f\"{photo_path}/Merged/{key}.jpg\")\n",
    "\n",
    "def make_all_images(video_path, photo_path, merged_df):\n",
    "    for _, row in merged_df.iterrows():\n",
    "        save_frame(row['frameId'], video_path, arrows=[get_arrow(row)])\n",
    "    merge_images(photo_path)\n",
    "    return \"Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNPPEcVrXQO_"
   },
   "outputs": [],
   "source": [
    "# Make df and pictures\n",
    "merged_df = merge_counter_and_tracker_df(counter_path, tracker_path, distance, orientation, notherly_correction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = merged_df[merged_df[\"frameId\"] != 277] #remove 237, 239, 274 and 277 from hogni_30fps_nosound_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_all_images(video_path, photo_path, merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6i0bNeScqxf"
   },
   "source": [
    "## Label pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnNFfKZhbMQo"
   },
   "outputs": [],
   "source": [
    "def show_pictures(photo_path, unique_id):\n",
    "    path = f\"{photo_path}/Merged\"\n",
    "    img = Image.open(f\"{path}/{unique_id}.jpg\")\n",
    "    return display(img)\n",
    "\n",
    "def label_photos(photo_path, merged_df, save_labels_path):\n",
    "    unique_id_set = set()\n",
    "    labels_dict = {}\n",
    "    for index, row in merged_df.iterrows():\n",
    "        unique_id_set.add(row[\"UniqueID\"])\n",
    "\n",
    "    with open(f'{save_labels_path}labels_dict_{file_name}.json', 'w') as f:\n",
    "        for i in unique_id_set:\n",
    "            display(merged_df[merged_df[\"UniqueID\"] == i])\n",
    "            show_pictures(photo_path, i)\n",
    "            path = int(input(\"Path: \"))\n",
    "            mode = int(input(\"Mode: \"))\n",
    "            labels_dict[i] = (path, mode)\n",
    "            json.dump(labels_dict, f)\n",
    "            clear_output(wait=True)\n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaEeCqomfiSC"
   },
   "outputs": [],
   "source": [
    "# Add tags to df\n",
    "# ------------------------------------------\n",
    "\n",
    "def add_labels_df(labels, merged_df):\n",
    "    for key, value in labels.items():\n",
    "        merged_df.loc[merged_df[\"UniqueID\"]==int(key), \"path\"]=value[0]\n",
    "        merged_df.loc[merged_df[\"UniqueID\"]==int(key), \"mode\"]=value[1]\n",
    "    return merged_df\n",
    "\n",
    "def save_labels(labels, file_name, save_labels_path):\n",
    "    json_file = json.dumps(labels)\n",
    "    f = open(f\"{save_labels_path}labels_{file_name}.json\",\"w\")\n",
    "    f.write(json_file)\n",
    "    f.close()\n",
    "    return \"saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = label_photos(photo_path, merged_df, save_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = add_labels_df(labels, merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save df and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_labels(labels, file_name, save_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.to_csv(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load saved DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"{save_labels_path}labels_{file_name}.json\",)\n",
    "labels = json.load(f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = add_labels_df(labels, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(['ObjectClass', 'name', 'confidence', 'counter_area', 'countingDirection', 'angle', 'bearing'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate augumented/ fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_data(X_list, y_list, n_data_point):\n",
    "#     \"\"\" Return db of generated data \"\"\"\n",
    "#     #\"bearing_1\", \"bearing_1\", \"velocity\"\n",
    "#     temp_x = []\n",
    "#     temp_y = []\n",
    "#     outer_loop_range = round(n_data_point/len(X_list))\n",
    "#     for i in range(outer_loop_range):\n",
    "#         for index, a in enumerate(X_list):\n",
    "#             noise = random.uniform(0, 5)\n",
    "#             temp_x.append([a[0] + noise, a[1]])\n",
    "#             temp_y.append(y_list[index])\n",
    "#     return temp_x, temp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake data from imagined paths\n",
    "# intersection_paths = {1:(270,270),2:(360,360),3:(90,90),4:(180,180),\n",
    "#                       5:(270,0),6:(360,90),7:(90,180),8:(180,270),\n",
    "#                       9:(180,90),10:(270,180),11:(360,270),12:(90,360),\n",
    "#                       13:(315,315),14:(45,45),15:(135,135),16:(225,225)}\n",
    "\n",
    "# def make_data(intersection_paths, n_samples, classes):\n",
    "#     x_data = []\n",
    "#     y_data = []\n",
    "    \n",
    "#     for i in range(n_samples):\n",
    "#         for key, value in intersection_paths.items():\n",
    "#             temp_x = []\n",
    "#             #temp_y = np.zeros((len(intersection_paths),), dtype=int)\n",
    "#             #temp_random = [(random.random()), (random.random() * 5), (random.random() * 10)]\n",
    "#             operator_functions = {\n",
    "#                 \"+\": lambda a, b: a + b,\n",
    "#                 \"-\": lambda a, b: a - b\n",
    "#             }\n",
    "#             operator = random.choice([\"+\", \"-\"])\n",
    "#             if operator == \"x\":\n",
    "#                 x_1 = operator_functions[operator]((random.uniform(0, 5)), (value[0]))\n",
    "#                 x_2 = operator_functions[operator]((random.uniform(0, 5)), (value[1]))\n",
    "#             else:\n",
    "#                 x_1 = operator_functions[operator]((value[0]), (random.uniform(0, 5)))\n",
    "#                 x_2 = operator_functions[operator]((value[1]), (random.uniform(0, 5)))\n",
    "                \n",
    "#             if x_1 > 360:\n",
    "#                 x_1 = x_1 - 360\n",
    "#             if x_1 <= 0:\n",
    "#                 x_1 = x_1 + 360\n",
    "#             if x_2 > 360:\n",
    "#                 x_2 = x_2 - 360\n",
    "#             if x_2 <= 0:\n",
    "#                 x_2 = x_2 + 360            \n",
    "#             temp_x.append(x_1)\n",
    "#             temp_x.append(x_2)\n",
    "#             #temp_y.append(int(key))\n",
    "#             x_data.append(temp_x)\n",
    "#             y_data.append(int(key))\n",
    "#     return x_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_xy_data(df, classes):\n",
    "#     X_vector_list = []\n",
    "#     y_vector_list = []\n",
    "#     temp_vector = []\n",
    "#     temp_id = 0\n",
    "#     for index, row in df.iterrows():\n",
    "#         if temp_id == 0:\n",
    "#             temp_id = row[\"UniqueID\"]\n",
    "#             temp_vector.append(row[\"adjusted_true_heading\"])\n",
    "#             #zeros =  np.zeros((classes,), dtype=int)\n",
    "#             #zeros[int(row[\"path\"])] = 1\n",
    "#             #zeros[int(-row[\"mode\"])] = 1\n",
    "#             y_vector_list.append(int(row[\"path\"]))\n",
    "#         else:\n",
    "#             if row[\"UniqueID\"] == temp_id:\n",
    "#                 temp_vector.append(row[\"adjusted_true_heading\"])\n",
    "#                 #temp_vector.append(row[\"velocity\"])\n",
    "#         if len(temp_vector) == 2: #Change to 3 if speed is desired.\n",
    "#             X_vector_list.append(temp_vector)\n",
    "#             temp_vector = []\n",
    "#             temp_id = 0\n",
    "#     return X_vector_list, y_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Real Data\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X_vector_list, y_vector_list, test_size=0.33, random_state=42, shuffle=True)\n",
    "# X_list, y_list= create_xy_data(merged_df, 16)\n",
    "# split_real_X_train, split_real_X_test, split_real_y_train, split_real_y_test = train_test_split(X_list, y_list, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# # Augumented Data\n",
    "# #X_train, y_train = gen_data(split_real_X_train, split_real_y_train, 500)\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "# # Fake data\n",
    "# fake_X_list, fake_y_list = make_data(intersection_paths, 1000, 16)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(fake_X_list, fake_y_list, test_size=0.33, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From train/ test split\n",
    "# X_train = tf.constant(X_train, dtype=tf.float32)\n",
    "# X_test = tf.constant(X_test, dtype=tf.float32)\n",
    "\n",
    "# y_train = tf.one_hot(y_train, classes)\n",
    "# y_test = tf.one_hot(y_test, classes)\n",
    "\n",
    "# # For test on real data\n",
    "# real_X_train = tf.constant(X_list, dtype=tf.float32)\n",
    "# real_y_train = tf.one_hot(y_list, classes)\n",
    "\n",
    "# real_X_test = tf.constant(X_list, dtype=tf.float32)\n",
    "# real_y_test = tf.one_hot(y_list, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.InputLayer(input_shape=(2,)),\n",
    "#     tf.keras.layers.Dense(30, activation='relu'),\n",
    "#     tf.keras.layers.Dense(20, activation='relu'),\n",
    "#     tf.keras.layers.Dense(16, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss=loss_fn,\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(real_X_test,  real_y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# kmeans = KMeans(n_clusters=3, random_state=0).fit(X_train)\n",
    "# kmeans.labels_\n",
    "\n",
    "# kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eucladean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance\n",
    "\n",
    "# predictions = []\n",
    "\n",
    "# for i in X_list:\n",
    "#     closest = [0, 0]\n",
    "#     for index, (key, value) in enumerate(intersection_paths.items()):\n",
    "#         dst = distance.euclidean(value, i)\n",
    "#         if index == 0:\n",
    "#             closest[0] = key\n",
    "#             closest[1] = dst\n",
    "#         if closest[1] > dst:\n",
    "#             closest[0] = key\n",
    "#             closest[1] = dst\n",
    "#     predictions.append(closest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance.euclidean(predictions, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomlist = []\n",
    "# for i in range(len(y_list)):\n",
    "#     n = random.randint(1,16)\n",
    "#     randomlist.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance.euclidean(randomlist, y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Example - Homography Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_cyclist_contact_coordiantes(df):\n",
    "    df['y'] = df['y'] + df['h']/2\n",
    "    df['x'] = df['x'] + df['w']/2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_df = create_tracker_df(tracker_path)\n",
    "tracker_df = project_cyclist_contact_coordiantes(tracker_df)\n",
    "#tracker_df.index.name = \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization, x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tracker_df.groupby(\"UniqueID\")[\"x\"].rolling(20, min_periods=1).mean().to_frame(name = \"mean_x\").droplevel(\"UniqueID\")\n",
    "tracker_df = tracker_df.join(df)\n",
    "df = tracker_df.groupby(\"UniqueID\")[\"y\"].rolling(20, min_periods=1).mean().to_frame(name = \"mean_y\").droplevel(\"UniqueID\")\n",
    "tracker_df = tracker_df.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_df = tracker_df[tracker_df.groupby(\"UniqueID\")[\"UniqueID\"].transform(\"size\") > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_src = cv2.imread('25.jpg')\n",
    "\n",
    "pts_src = np.array([[1110, 841],\n",
    "                   [1484, 934],\n",
    "                   [1839, 692],\n",
    "                   [1646, 670]])\n",
    "\n",
    "im_dst = cv2.imread('Screenshot 2021-03-16 at 21.54.45.png')\n",
    "\n",
    "pts_dst = np.array([[1676, 1802],\n",
    "                   [1787, 1929],\n",
    "                   [2460, 1505],\n",
    "                   [2355, 1371]])\n",
    "\n",
    "\n",
    "matrix, status = cv2.findHomography(pts_src, pts_dst)\n",
    "\n",
    "im_out = cv2.warpPerspective(im_src, matrix, (im_dst.shape[1],im_dst.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_colour_list(tracker_df, x, y):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    colour_list =[]\n",
    "\n",
    "    for index, row in tracker_df.iterrows():\n",
    "        if row[\"bearing\"] > 315 and row[\"bearing\"] <= 45:\n",
    "            x_list.append(int(round(row[f\"{x}\"])))\n",
    "            y_list.append(int(round(row[f\"{y}\"])))\n",
    "            colour_list.append((0,0,255))\n",
    "        elif row[\"bearing\"] > 45 and row[\"bearing\"] <= 135:\n",
    "            x_list.append(int(round(row[f\"{x}\"])))\n",
    "            y_list.append(int(round(row[f\"{y}\"])))\n",
    "            colour_list.append((0,255,0))\n",
    "        elif row[\"bearing\"] > 135 and row[\"bearing\"] <= 225:\n",
    "            x_list.append(int(round(row[f\"{x}\"])))\n",
    "            y_list.append(int(round(row[f\"{y}\"])))\n",
    "            colour_list.append((255,0,0))\n",
    "        elif row[\"bearing\"] > 225 and row[\"bearing\"] <= 315:\n",
    "            x_list.append(int(round(row[f\"{x}\"])))\n",
    "            y_list.append(int(round(row[f\"{y}\"])))\n",
    "            colour_list.append((255,255,0))\n",
    "    return x_list, y_list, colour_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cyclist_paths(x_list, y_list, colour_list, matrix, im_out):\n",
    "    for index, i in enumerate(x_list):\n",
    "        p = (i, y_list[index])\n",
    "        px = (matrix[0][0]*p[0] + matrix[0][1]*p[1] + matrix[0][2]) / ((matrix[2][0]*p[0] + matrix[2][1]*p[1] + matrix[2][2]))\n",
    "        py = (matrix[1][0]*p[0] + matrix[1][1]*p[1] + matrix[1][2]) / ((matrix[2][0]*p[0] + matrix[2][1]*p[1] + matrix[2][2]))\n",
    "        p_after = (int(px), int(py))\n",
    "\n",
    "        # Draw the new point\n",
    "        cv2.circle(im_out,p_after, 5, colour_list[index], -1)\n",
    "    return im_out\n",
    " \n",
    "# Change to change picture of overlay\n",
    "x_list, y_list, colour_list = x_y_colour_list(tracker_df, \"mean_x\", \"mean_y\")\n",
    "im_out = plot_cyclist_paths(x_list , y_list, colour_list, matrix, im_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(24,24))\n",
    "plt.imshow(im_out)\n",
    "#plt.savefig('Video_overlay.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_id_set = set()\n",
    "#for index, row in tracker_df.iterrows():\n",
    "#    unique_id_set.add(row[\"UniqueID\"])\n",
    "\n",
    "#for i in unique_id_set:\n",
    "#    #im_out = cv2.warpPerspective(im_src, matrix, (im_dst.shape[1],im_dst.shape[0]))\n",
    "#    df = tracker_df[tracker_df[\"UniqueID\"] == i]\n",
    "#    x_list, y_list, colour_list = x_y_colour_list(df)\n",
    "#    im_out = plot_cyclist_paths(x_list , y_list, colour_list, matrix, im_dst)\n",
    "#    plt.figure(figsize=(24,24))\n",
    "#    plt.imshow(im_out)\n",
    "#    plt.show()\n",
    "#    input(\"Press Enter to continue...\")\n",
    "#    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.savefig('Google_maps_overlay.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picture clicking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "   \n",
    "def click_event(event, x, y, flags, params):\n",
    "  \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "\n",
    "        print(x, ' ', y)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, str(x) + ',' +\n",
    "                    str(y), (x,y), font,\n",
    "                    1, (255, 0, 0), 2)\n",
    "        cv2.imshow('image', img)\n",
    "\n",
    "img = cv2.imread('25.jpg', 1)\n",
    "cv2.imshow('image', img)\n",
    "cv2.setMouseCallback('image', click_event)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP+20xGmDFF1LJlX58YocUi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1eqU3ZQVqOVHpaishPC0gHhkHDdSB_c3F",
   "name": "cyclist_behaviour.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
