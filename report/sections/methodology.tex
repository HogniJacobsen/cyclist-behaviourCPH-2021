The approach takes place in three discrete phases: \textit{data collection}, \textit{data processing} and \textit{analysis}, as summarized
by figure \ref{pipeline}.

\ \\ 
\raggedbottom
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{pipeline} 
\end{tabular}
\captionof{figure}{Overview of implementation} 
\label{pipeline}

\subsection{Data collection}
\ \\ 
\noindent
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{dybbølsbro}
\end{tabular}
\captionof{figure}{Dybbølsbro/Ingerslevgade intersection \\ \textit{Image source: Københavns Kommune}} 
\label{intersection_overview}
\

Our work is centered around the Dybbølsbro / Ingerslevgade intersection south-west of Copenhagen
city center. The intersection faces several challenges, producing \textit{'conflicts, unsafe situations, illegal 
road user behavior, and great dissatisfaction among road users at the intersection'} (\cite{CPHpost_2021}).
From the south, the intersection connects to the Dybbølsbro bridge, which features a bi-directional cycle path, two
lanes for vehicle traffic, and pedestrians from the nearby train station. 
This is an unusual setup in Copenhagen, as most streets feature unidirectional cycle paths 
following vehicle traffic. Therefore, south-bound cyclists coming from the north (Skelbækgade) have to perform a two-phase 
turn to continue over the bridge. Firstly by heading straight over the intersection but then stopping and performing a left turn to 
continue. This leads to high congestion at the south-west corner of the intersection with many cyclists looking for a shortcut across. 
Due to its issues and size as well as previous analysis conducted on the intersection, we found it to be a good baseline for experimentation.

\subsubsection{Recording location}
There are two considerations to take into account when recording an intersection for video analysis.
\ \\ \\

\textbf{Intersection size} \\
The size of the intersection will determine the camera setup needed. Larger intersections require a two-camera setup, 
however, this might not be optimal for intersections much larger than the Dybbølsbro intersection. 
Initially, we used a single-camera setup but found that while we had a good view of the entire intersection, 
we lost tracking data when the view of the cyclists' on the opposite side of the intersection was obstructed by vehicles.
\ \\

\textbf{Camera mounting points} \\
We tried three different mounting locations shown by the red and purple circles in figure \ref{intersection_overview}. 
We faced several difficulties with camera mounting at the intersection, including camera stability, 
obstructions of the camera view, and the height of the mounting positions. 
With some of our camera mounts, stability was impacted by wind and cause the camera to sway, resulting in skewed detections later on.
To cover the entire intersection with adequate coverage, we selected a two-camera setup on opposite sides of the intersection 
(purple circles on figure \ref{intersection_overview}).

\subsubsection{Camera setup}
Any camera device with an adequate field-of-view (FOV) to capture the intersection that records in HD (1280×720 resolution) at a constant 
frame rate (at least 10FPS), and that allows remote viewing of its viewfinder will suffice. 
However, having tested a Raspberry Pi recording setup (including a camera module, external battery, and an LCD touch screen), 
we would not recommend a self-built recording setup. Although the Raspberry Pi setup meets the above-mentioned technical requirements, 
the challenges lie in the usability of such a setup. 
It introduces unnecessary complexities by having to manually assemble the hardware and install the necessary software, as well as 
having surety of its operation in the field.
Alternative devices optimized for video recording, such as mobile phones or action cameras, are more suitable.
These devices typically offer methods of remotely controlling and viewing their software interface from 
another device such as a PC or smartphone. 
This is useful when mounting the camera and adjusting its position to get a good view of the intersection. 
While also possible with a Raspberry Pi, it is less stable compared to using smartphones or action cameras.
Hence, we chose to use smartphones for recording. We used two different smartphones, an \textit{LG G6} and a \textit{Samsung Galaxy S7}; however, we recommend using the same recording devices. Using the same device brings consistency when applying this method.

% Along with meeting the basic requirements they also have Ingress Protection ratings, 
% commonly referred to as waterproofing. This should be considered if rain is a possibility.
\ \\

% Given a recording location, we can use (eq. \ref{eq:1}) to calculate the FOV a camera needs to capture an intersection.
% If $\theta > FOV$, then the FOV is too small.
% \color{red}
% Note: Label images and make them more understandable.
% \color{black}

% \begin{equation}
%     \theta = tan^-1(\frac{\frac{width}{2}}{adjacent}) * 2\label{eq:1}
%   \end{equation}

% \ \\ 
% \raggedbottom
% \begin{tabular}{@{}cc}
% \includegraphics[width=1.0\columnwidth]{location} 
% \end{tabular}
% \captionof{figure}{Camera location}
% \label{Camera location}
% \

% Battery life and storage capacity should also be considered depending on the amount of intended recording. 
% With regards to storage, a reasonable estimate for video size would be 149MB per 1 min of FullHD (1920*1080) at 30FPS. Storage should be selected
% with the intended amount of recording time.

\ \\ 
\raggedbottom
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{contraptions}
\end{tabular}
\captionof{figure}{Camera mounts} 
\label{camera_mounts}
\

We used two different camera mounts at the intersection (figure \ref{camera_mounts}). One includes a flexible arm that can be clammed onto infrastructure such as light poles.
The second mounting solution was a self-built holder made out of cardboard and a hook, allowing us to easily mount it at high spots using a long extension rod.
Selecting an appropriate mounting height is a trade-off between maximizing the captured area of the intersection and having a frontal view of traffic.
We aimed to have cameras mounted at least 3-4 meters above the ground.

\subsection{Data Processing}
An overview of our proposed data pipelines are shown in figure \ref{data}. 
The implementation can be found in the following \href{https://github.com/edibegovic/cyclist-behaviourCPH-2021}{git repository}.
\ \\

\color{red}
\raggedbottom
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{data_flow.png} 
\end{tabular}
\captionof{figure}{Generalized Data Pipelines}
\label{data}
\raggedbottom
\color{black}

\subsubsection{Video synchronization}
An important preprocessing step is to cut the different video sources to the same timestamp.
Should the video have different frame rates or resolutions, then these should be processed to match.

\subsubsection{Camera calibration}
Cameras often suffer from optical aberration where straight lines appear bent, especially noticeable in the edges of an image. 
This can be observed on wide-angle camera lenses, such as on the LG G6 we used.
The specific type is \textit{positive radial distortion} as shown in figure \ref{distortion_types}, with lines curving outwards in a barrel shape.
These aberrations are a result of the curved shape of the camera lens.
\ \\ 

\raggedbottom
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{calibration_radial_distortion} 
\end{tabular}
\captionof{figure}{Types of lens distortion}
\label{distortion_types}
\

There is also a possibility of further distortion if the camera sensor and the lens are not parallel, also known as \textit{tangential distortion}.
Ideally, there should be no radial nor tangential distortion.
\ \\
It became apparent when merging data from multiple camera sources that lens distortion affected the alignment of data points.
This is shown in figure \ref{joined_distortion}, where trajectories detected from one camera source do not match the other.

\raggedbottom
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{calibration.png}
\end{tabular}
\captionof{figure}{Misaligned sources - before and after calibration}
\label{joined_distortion}
\

To correct for the distortion we made use of OpenCVs camera calibration toolbox\footnote{\href{https://github.com/opencv/opencv}{OpenCV}}.

In order to correct for distortion, the camera matrix and distortion coefficients are needed. To get the camera matrix and distortion coefficients, we used the calibrate script provided with OpenCV,
passing several snapshots of a calibration object taken with the device to be calibrated. The calibration object is a black-white chessboard pattern.
We passed 20+ sample images of the calibration object in different orientations, angles, and positions in the frame. 
OpenCV precesses the images and returns a camera matrix and the distortion coefficients.
\ \\

See appendix B for examples of before and after distortion correction.

\subsubsection{Object detection}
We found that our offline OpenDataCam setup using an Nvidia Jetson NX device worked well for detecting cars, 
but did not perform as well on cyclists. 
We, therefore, implemented our object detection setup using YOLOv5. YOLOv5, trained on the same COCO dataset\footnote{\href{https://cocodataset.org}{COCO dataset}}
as previous YOLO models, showed a marked improvement on the detection of
cyclists.
\ \\ 

We ran  YOLOv5 using the largest model available, yolov5x6.
The model was run on an NVIDIA Tesla P100 GPU (CUDA enabled) with 16GB of memory. The model was set only to detect cyclists and to process
video at a resolution of 1280 pixels.
\ \\
 
With this setup, videos were processed at ~10 FPS. This results in a total processing time of circa ~4.5 hours for a ~1.5-hour video.
Predicted objects are represented as bounding boxes with the output being represented as $[[frame id][xmin][ymin][xmax][ymax][confidence]]$.

\subsubsection{Projection}
To achieve the 'birds-eye view' of the intersection, we needed to explore methods of 
transforming the data from the camera views.  
\ \\

A homography matrix is a transformation matrix between two planes (\cite{hartley_zisserman_2004}).
It will allow us to transform and plot the cyclist movements onto an aerial photograph of 
the intersection.
\ \\ 

\noindent
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{projection_figure} 
\end{tabular}
\captionof{figure}{Projection - ground control points}
\label{projection_figure}
\

To calculate the homography matrix, we need to solve the system of linear equations, $P = HQ$,
$P$ being points in a polygon on the source image (Figure \ref{projection_figure}, video view), $Q$ the corresponding polygon on the destination image (Figure \ref{projection_figure}, aerial view). 
$H$ being the $3 \times 3$ homography matrix.
\ \\

Before we can project the cyclist onto an aerial view, we first need to calculate the 2D coordinates of their contact points with the road surface.
Given equation \ref{eqn:eq1}, we can calculate the contact points using the objects bounding box coordinates.

\begin{equation}
\label{eqn:eq1}
\begin{array}{l}
x = x_\mathrm{min} + \frac{(x_\mathrm{max} - x_\mathrm{min})}{2} \\
y = y_\mathrm{min}
\end{array}
\end{equation}
\ \\

After solving the homography matrix, we can perform the projection.
This is achieved by applying the cyclist's contact coordinates, $P$, and the homography matrix to the equation $P = HQ$. 
This returns the corresponding pixel coordinates of the cyclist on the destination image.

\subsubsection{Merging sources}
As we are using multiple video sources, there will be an overlap of the detected cyclists. 
That is, the trajectory of the same cyclist will be captured by multiple cameras for specific areas. 
To merge the trajectories from the video sources, we take a naive approach. As the cameras are set up on
opposite sides of the intersection, we cut the video sources in half along the mid-point between
the two cameras along the intersection to join them.

\ \\ 
\noindent
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{slice} 
\end{tabular}
\captionof{figure}{Cutting of video sources}
\label{slice}
\

\subsubsection{Tracking}

In order to connect detections into trajectories of individual cyclist, we apply 
SORT\footnote{\href{https://github.com/abewley/sort}{SORT algorithm}}, \textit{simple online and real-time tracking}, as initially described in (\cite{Bewley2016_sort}). 
SORT aims to address multiple object tracking (MOT), where objects across frames need to be connected. 
\ \\ 

SORT uses Intersection over Union (IOU) as a matching criterion for detections between frames. The IOU is 
calculated by dividing the overlap between the bounding boxes by the union of the bounding boxes. 
To identify objects between frames the IOU of a detection \textit{B} in frame $t + 1$ is compared with 
predicted position, using Kalman Filters, in frame $t + 1$ of detection \textit{A} from frame $t$, this 
is done for all detections between frames. The Hungarian algorithm is then applied to perform the optimal 
matching of detection. 
\ \\ 

In our implementation, we define arbitrary bounding boxes around the center point of the projected detections; 
we used a $10 \times 10$ pixel bounding box. Using arbitrary bounding boxes results from the plane of 
the bounding boxes from the object detection being on a different plane to that of the road's surface.

\subsection{Analysis}

\subsubsection{Web application}
To interact with the tools implemented, we built a simple web application using the Dash\footnote{\href{https://plotly.com/dash/}{Dash library}} for Python. 
Its main component consists of a satellite/aerial view of the chosen intersection with the trajectories of cyclists plotted as short-lived tracks. 
One can then explore the timeline by either explicitly referencing a timestamp or frame, using a slider, or jumping in set increments.
Below the aerial view is a set of image viewers showing the current frame from all the cameras with markers around selected cyclists.

\ \\ 
\raggedbottom
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{webapp} 
\end{tabular}
\captionof{figure}{Web application}
\label{webapp}
\

To gain insight into specific areas of the intersection, observations can be filtered by drawing a mask on the aerial view. 
When an area is selected, all cyclists passing it will be listed in a side panel, from which one can jump directly to the matching timestamp.
\
We also implemented counting lines (as found in OpenDataCam) are also implemented, giving the user the ability to immediately see the number of 
cylists passing certain parts of the intersection in either direction.
\ \\ \\

\textbf{Future work} \\
Currently, the data preprocessing is not part of the application, thereby functioning more as a \textit{proof of concept}.
It would be relatively easy to connect the data processing pipeline to the web interface, thus simply making the user 
select video sources, map the \textit{ground control points} (for aerial projection), and potentially tweaking other hyper-parameters. 

\subsubsection{Rainbow tracks}

\ \\ 
\noindent
\begin{tabular}{@{}cc}
\includegraphics[width=1.0\columnwidth]{rainbow.png} 
\end{tabular}
\captionof{figure}{Rainbow tracks}
\label{Rainbow}
\

To find aggregated desire lines from the data we took an approach which we call 'Rainbow tracks', as
seen in figure \ref{Rainbow}. 
This involves coloring tracks by the bearing between consecutive points in each trajectory. 
After calculating the bearing, we then get a color from a gradient color wheel. 
This approach has the added benefit of encoding direction into each track.
\ \\ 

% \begin{equation}
%   UniqueID_i = [(x_1, y_1)...(x_a+1, y_a+1)]\label{eq:3}
% \end{equation}

