{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xuKObBsQaxdR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import json \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pcOEfWXb_jpY"
   },
   "outputs": [],
   "source": [
    "# Variables.\n",
    "# ------------------------------------------\n",
    "\n",
    "who_is_running_this_code = \"hogni\"\n",
    "\n",
    "file_name = \"edi_30fps_nosudio_full\"\n",
    "parent_parth = f\"/Users/{who_is_running_this_code}/Library/Mobile Documents/com~apple~CloudDocs/Bachelor Project/Videos/09032021/\"\n",
    "\n",
    "counter_path = f\"{parent_parth}Data/{file_name}/counterData_{file_name}.csv\"\n",
    "tracker_path = f\"{parent_parth}Data/{file_name}/tracker{file_name}.csv\"\n",
    "video_path = f\"{parent_parth}Processed/{file_name}.mp4\"\n",
    "photo_path = f\"{parent_parth}Photos/{file_name}\"\n",
    "save_labels_path = f\"{parent_parth}Data/{file_name}/\"\n",
    "db_path = f\"{parent_parth}Data/{file_name}/{file_name}_db.csv\"\n",
    "distance = 2.6\n",
    "orientation = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ykoo9V71axru"
   },
   "outputs": [],
   "source": [
    "# Velocity Calculations\n",
    "# ------------------------------------------\n",
    "\n",
    "def calculate_velocity(distance, df):\n",
    "    \"\"\"Velocity Calculations: v = S/t \"\"\"\n",
    "    velocit_dictionary = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if row[4] in velocit_dictionary.keys():\n",
    "            velocit_dictionary[row[4]].append(row[1])\n",
    "        else:\n",
    "            velocit_dictionary[row[4]] = [row[1]]\n",
    "\n",
    "    for key, value in velocit_dictionary.copy().items():\n",
    "        if value[0] > value[1]:\n",
    "            difference = (value[0]-value[1])\n",
    "        else:\n",
    "            difference = (value[1]-value[0])\n",
    "        velocit_dictionary[key] = difference\n",
    "\n",
    "    for key, value in velocit_dictionary.copy().items():\n",
    "        t = velocit_dictionary[key].total_seconds()\n",
    "        if t == 0:\n",
    "            t = 1\n",
    "            velocit_dictionary[key] = distance/t\n",
    "        else:\n",
    "            velocit_dictionary[key] = distance/t\n",
    "    return velocit_dictionary\n",
    "\n",
    "# Absolute Heading \n",
    "# ------------------------------------------\n",
    "\n",
    "def calculate_true_heading(frame_orientation, df):\n",
    "    true_heading_dictionary = {}\n",
    "    for index, row in df.iterrows():\n",
    "        true_heading = row[5] + frame_orientation\n",
    "        if true_heading > 360:\n",
    "            true_heading = true_heading - 360\n",
    "        true_heading_dictionary[row[0]] = true_heading\n",
    "    return true_heading_dictionary\n",
    "\n",
    "# Init DataFrames\n",
    "# ------------------------------------------\n",
    "\n",
    "# Counter\n",
    "def create_counter_df(path, distance, orientation):\n",
    "    \"\"\"Read and format counter df\"\"\"\n",
    "    columns = [\"frameId\", \"timestamp\", \"counter_area\", \"ObjectClass\", \n",
    "            \"UniqueID\", \"bearing_og\", \"countingDirection\", \"angle\"]\n",
    "\n",
    "    counter = pd.read_csv(path, names=columns)\n",
    "    counter = counter[counter.duplicated(subset=[\"UniqueID\"], keep=False)].sort_values(\"UniqueID\")\n",
    "    counter['timestamp'] = pd.to_datetime(counter['timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    counter = fix_df(counter)\n",
    "    counter = add_velocity_to_df(distance, counter)\n",
    "    counter = add_true_heading_to_df(orientation, counter)\n",
    "    return counter\n",
    "\n",
    "def add_velocity_to_df(distance, counter_df):\n",
    "    \"\"\" Calculates speeds (and append to df) speeds: {UniqueID: velocity} \"\"\"\n",
    "    speeds = calculate_velocity(distance, counter_df)\n",
    "    for key, value in speeds.items():\n",
    "        counter_df.loc[counter_df[\"UniqueID\"]==key, \"velocity\"]=value\n",
    "    return counter_df\n",
    "\n",
    "def add_true_heading_to_df(orientation, counter_df):\n",
    "    \"\"\" Calculates true heading (and append to df): {frameId: true_heading} \"\"\"\n",
    "    true_heading = calculate_true_heading(orientation, counter_df)\n",
    "    for key, value in true_heading.items():\n",
    "        counter_df.loc[counter_df[\"frameId\"]==key, \"true_heading\"]=value\n",
    "    return counter_df\n",
    "\n",
    "# Tracker\n",
    "def create_tracker_df(tracker_path):\n",
    "    with open(tracker_path) as f:\n",
    "        tracker = json.load(f)\n",
    "    tracker_flattend = pd.json_normalize(tracker, record_path='objects', meta=['frameId']).rename(columns={'id': 'UniqueID'})\n",
    "    tracker_flattend = tracker_flattend.drop_duplicates(subset=['UniqueID', 'frameId'], keep='first')\n",
    "    return tracker_flattend\n",
    "\n",
    "# Fix inaccurate data\n",
    "# ------------------------------------------\n",
    "\n",
    "def fix_df(counter_df):\n",
    "    error_dict = {}\n",
    "    for index, row in counter_df.iterrows():\n",
    "        if row[\"UniqueID\"] in error_dict:\n",
    "            error_dict[row[\"UniqueID\"]].append([row[\"frameId\"], row[\"bearing_og\"]])\n",
    "        else:\n",
    "            error_dict[row[\"UniqueID\"]] = [[row[\"frameId\"], row[\"bearing_og\"]]]\n",
    "\n",
    "    for key, value in error_dict.items():\n",
    "        if value[0][1] == 270 or value[1][1] == 270:\n",
    "            if value[0][1] == 270:\n",
    "                if value[1][1] < 150 and value[1][1] > 50:\n",
    "                   counter_df.loc[counter_df[\"frameId\"]==error_dict[key][0][0], \"bearing_og\"]=90\n",
    "            else:\n",
    "                if value[0][1] < 150 and value[0][1] > 50:\n",
    "                   counter_df.loc[counter_df[\"frameId\"]==error_dict[key][1][0], \"bearing_og\"]=90\n",
    "    return counter_df\n",
    "\n",
    "# Merge counter and tracker\n",
    "# ------------------------------------------\n",
    "\n",
    "def merge_counter_and_tracker_df(counter_path, tracker_path, distance, orientation):\n",
    "    counter_df = create_counter_df(counter_path, distance, orientation)\n",
    "    tracker_df = create_tracker_df(tracker_path)\n",
    "    return pd.merge(counter_df, tracker_df, on=[\"UniqueID\", \"frameId\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "O2pJ65_2a8aF"
   },
   "outputs": [],
   "source": [
    "# Image Generation\n",
    "# ------------------------------------------\n",
    "\n",
    "def save_frame(frame_number, source, arrows=None):\n",
    "    vc = cv2.VideoCapture(source)\n",
    "    vc.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    rval, frame = vc.read()\n",
    "    if arrows != None:\n",
    "        for a in arrows:\n",
    "            frame = cv2.arrowedLine(frame, a['start'], a['end'], (0,0,255), thickness=8, tipLength=0.6)\n",
    "        cv2.imwrite(f\"{photo_path}/{str(frame_number)}.jpg\", frame)\n",
    "\n",
    "def get_arrow(obj):\n",
    "    x, y = obj['x'], obj['y']\n",
    "    angle = obj['bearing_og']\n",
    "    speed = obj['velocity']\n",
    "    a = 30 * speed * math.sin(math.radians(angle))\n",
    "    b = 30 * speed * math.cos(math.radians(angle))\n",
    "    return {'start': (int(x), int(y)), 'end': (int(x+a), int(y+b))}\n",
    "\n",
    "# Stich images together.\n",
    "# ------------------------------------------\n",
    "\n",
    "def get_image_pairs(merged_df):\n",
    "    image_pairs_dict = {}\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if row[4] in image_pairs_dict:\n",
    "            image_pairs_dict[row[\"UniqueID\"]].append(row[\"frameId\"])\n",
    "        else:\n",
    "            image_pairs_dict[row[\"UniqueID\"]] = [row[\"frameId\"]]\n",
    "    return image_pairs_dict\n",
    "\n",
    "def merge_images(photo_path):\n",
    "    image_pairs_dict = get_image_pairs(merged_df)\n",
    "    for key, value in image_pairs_dict.items():\n",
    "        image1 = Image.open(f\"{photo_path}/{value[0]}.jpg\")\n",
    "        image2 = Image.open(f\"{photo_path}/{value[1]}.jpg\")\n",
    "\n",
    "        (width1, height1) = image1.size\n",
    "\n",
    "        result_width = width1 * 2\n",
    "        result_height = height1\n",
    "\n",
    "        result = Image.new('RGB', (result_width, result_height))\n",
    "        result.paste(im=image1, box=(0, 0))\n",
    "        result.paste(im=image2, box=(width1, 0))\n",
    "        result.save(f\"{photo_path}/Merged/{key}.jpg\")\n",
    "\n",
    "def make_all_images(video_path, photo_path, merged_df):\n",
    "    for _, row in merged_df.iterrows():\n",
    "        save_frame(row['frameId'], video_path, arrows=[get_arrow(row)])\n",
    "    merge_images(photo_path)\n",
    "    return \"Completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make df and pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rNPPEcVrXQO_"
   },
   "outputs": [],
   "source": [
    "# Make df and pictures\n",
    "#merged_df = merge_counter_and_tracker_df(counter_path, tracker_path, distance, orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_all_images(video_path, photo_path, merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6i0bNeScqxf"
   },
   "source": [
    "## Tag pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YnNFfKZhbMQo"
   },
   "outputs": [],
   "source": [
    "def show_pictures(photo_path, unique_id):\n",
    "    path = f\"{photo_path}/Merged\"\n",
    "    img = Image.open(f\"{path}/{unique_id}.jpg\")\n",
    "    return display(img)\n",
    "\n",
    "def label_photos(photo_path, merged_df):\n",
    "    unique_id_set = set()\n",
    "    labels_dict = {}\n",
    "    for index, row in merged_df.iterrows():\n",
    "        unique_id_set.add(row[\"UniqueID\"])\n",
    "    \n",
    "    for i in unique_id_set:\n",
    "        show_pictures(photo_path, i)\n",
    "        path = int(input(\"Path: \"))\n",
    "        mode = int(input(\"Mode: \"))\n",
    "        labels_dict[i] = (path, mode)\n",
    "        clear_output(wait=True)\n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "NaEeCqomfiSC"
   },
   "outputs": [],
   "source": [
    "# Add tags to df\n",
    "# ------------------------------------------\n",
    "\n",
    "def add_labels_df(labels, merged_df):\n",
    "    for key, value in labels.items():\n",
    "        merged_df.loc[merged_df[\"UniqueID\"]==key, \"path\"]=value[0]\n",
    "        merged_df.loc[merged_df[\"UniqueID\"]==key, \"mode\"]=value[1]\n",
    "    return merged_df\n",
    "\n",
    "def save_labels(labels, file_name, save_labels_path):\n",
    "    json_file = json.dumps(labels)\n",
    "    f = open(f\"labels_{file_name}.json\",\"w\")\n",
    "    f.write(json_file)\n",
    "    f.close()\n",
    "    return \"saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = label_photos(photo_path, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = add_labels_df(labels, merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_labels(labels, file_name, save_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.to_csv(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load saved DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(f\"{save_labels_path}labels_{file_name}.json\",)\n",
    "#labels = json.load(f) \n",
    "#f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = add_labels_df(labels, merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(X_vector_list, y_vector_list, n_data_point):\n",
    "    \"\"\" Return db of generated data \"\"\"\n",
    "    #\"bearing_1\", \"bearing_1\", \"velocity\"\n",
    "    temp_x = []\n",
    "    temp_y = []\n",
    "    outer_loop_range = round(n_data_point/len(X_vector_list))\n",
    "    for i in range(outer_loop_range):\n",
    "        for index, a in enumerate(X_vector_list):\n",
    "            noise = random.randrange(1, 5, 1)\n",
    "            noise = noise + random.random()\n",
    "            temp_x.append([a[0] + noise, a[1] + noise, a[2]])\n",
    "            temp_y.append(y_vector_list[index])\n",
    "    return temp_x, temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xy_data(df):\n",
    "    X_vector_list = []\n",
    "    y_vector_list = []\n",
    "    temp_vector = []\n",
    "    temp_id = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if temp_id == 0:\n",
    "            temp_id = row[\"UniqueID\"]\n",
    "            temp_vector.append(row[\"bearing\"])\n",
    "            #zeros = [0,0,0,0,0,0,0]\n",
    "            zeros = [0,0,0,0,0]\n",
    "            zeros[int(row[\"path\"])-1] = 1\n",
    "            #zeros[int(-row[\"mode\"])] = 1\n",
    "            y_vector_list.append(zeros)\n",
    "        else:\n",
    "            if row[\"UniqueID\"] == temp_id:\n",
    "                temp_vector.append(row[\"bearing\"])\n",
    "                temp_vector.append(row[\"velocity\"])\n",
    "        if len(temp_vector) == 3:\n",
    "            X_vector_list.append(temp_vector)\n",
    "            temp_vector = []\n",
    "            temp_id = 0\n",
    "    return X_vector_list, y_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_vector_list, y_vector_list, test_size=0.33, random_state=42)\n",
    "X_vector_list, y_vector_list= create_xy_data(merged_df)\n",
    "\n",
    "# Fake Data\n",
    "X_train, y_train = gen_data(X_vector_list, y_vector_list, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_vector_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(X_train, dtype=tf.float32)\n",
    "X_test = tf.constant(X_vector_list, dtype=tf.float32)\n",
    "\n",
    "y_train = tf.constant(y_train, dtype=tf.int16)\n",
    "y_test = tf.constant(y_vector_list, dtype=tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 345\n",
      "Trainable params: 345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "34/34 [==============================] - 0s 481us/step - loss: 11.8828 - accuracy: 0.5538\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - 0s 468us/step - loss: 2.6995 - accuracy: 0.3421\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - 0s 421us/step - loss: 1.3580 - accuracy: 0.3542\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - 0s 413us/step - loss: 0.8543 - accuracy: 0.4657\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - 0s 400us/step - loss: 0.6100 - accuracy: 0.5762\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - 0s 397us/step - loss: 0.4357 - accuracy: 0.5782\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - 0s 389us/step - loss: 0.4138 - accuracy: 0.5978\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - 0s 406us/step - loss: 0.3735 - accuracy: 0.6245\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - 0s 396us/step - loss: 0.3315 - accuracy: 0.6679\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - 0s 412us/step - loss: 0.3367 - accuracy: 0.8129\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - 0s 402us/step - loss: 0.3285 - accuracy: 0.8077\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - 0s 401us/step - loss: 0.2682 - accuracy: 0.8853\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - 0s 402us/step - loss: 0.2931 - accuracy: 0.9174\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - 0s 391us/step - loss: 0.2771 - accuracy: 0.9119\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - 0s 386us/step - loss: 0.2492 - accuracy: 0.9067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13cb70460>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 0s - loss: 0.2459 - accuracy: 0.9211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2459195852279663, 0.9210526347160339]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP+20xGmDFF1LJlX58YocUi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1eqU3ZQVqOVHpaishPC0gHhkHDdSB_c3F",
   "name": "cyclist_behaviour.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
